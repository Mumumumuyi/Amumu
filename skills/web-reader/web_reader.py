#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
web-reader - ç½‘é¡µå†…å®¹è¯»å–ä¸æ•´ç†å·¥å…·
åŠŸèƒ½: ä»ç½‘å€è¯»å–ç½‘é¡µå†…å®¹ï¼Œæ•´ç†å¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹
ç”¨æ³•: python web_reader.py "https://example.com"
"""

import sys
import os
import json
import argparse
from datetime import datetime
from pathlib import Path

try:
    import requests
    from bs4 import BeautifulSoup
    from urllib.parse import urlparse
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–åº“: {e}")
    print("è¯·å®‰è£…: pip install requests beautifulsoup4")
    sys.exit(1)


class WebReader:
    """ç½‘é¡µå†…å®¹è¯»å–å™¨"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def fetch_page(self, url):
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            response = self.session.get(url, timeout=30, allow_redirects=True)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            return response.text, response.url
        except Exception as e:
            raise Exception(f"è·å–ç½‘é¡µå¤±è´¥: {e}")

    def parse_content(self, html, base_url):
        """è§£æç½‘é¡µå†…å®¹"""
        soup = BeautifulSoup(html, 'html.parser')

        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe', 'noscript']):
            tag.decompose()

        # æå–ä¿¡æ¯
        data = {
            'title': self._get_title(soup),
            'url': base_url,
            'meta': self._get_meta(soup),
            'headings': self._get_headings(soup),
            'main_content': self._get_main_content(soup),
            'links': self._get_links(soup, base_url),
            'images': self._get_images(soup, base_url),
        }

        return data

    def _get_title(self, soup):
        """è·å–é¡µé¢æ ‡é¢˜"""
        title_tag = soup.find('title')
        return title_tag.get_text(strip=True) if title_tag else "æ— æ ‡é¢˜"

    def _get_meta(self, soup):
        """è·å–å…ƒæ•°æ®"""
        meta = {}
        for tag in soup.find_all('meta'):
            name = tag.get('name') or tag.get('property')
            if name in ['description', 'keywords', 'author']:
                meta[name] = tag.get('content', '')
        return meta

    def _get_headings(self, soup):
        """è·å–æ ‡é¢˜ç»“æ„"""
        headings = []
        level_map = {'h1': 1, 'h2': 2, 'h3': 3, 'h4': 4, 'h5': 5, 'h6': 6}
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            headings.append({
                'level': level_map.get(tag.name, 0),
                'text': tag.get_text(strip=True)
            })
        return headings

    def _get_main_content(self, soup):
        """è·å–ä¸»è¦å†…å®¹"""
        # ä¼˜å…ˆè·å– main æˆ– article æ ‡ç­¾
        main_content = (soup.find('main') or
                       soup.find('article') or
                       soup.find('div', class_=lambda x: x and ('content' in x.lower() or 'article' in x.lower())))

        if not main_content:
            main_content = soup.body or soup

        # æå–æ®µè½æ–‡æœ¬
        paragraphs = []
        for p in main_content.find_all('p'):
            text = p.get_text(strip=True)
            if text:
                paragraphs.append(text)

        return '\n\n'.join(paragraphs) if paragraphs else "æœªæ‰¾åˆ°ä¸»è¦å†…å®¹"

    def _get_links(self, soup, base_url):
        """è·å–é¡µé¢é“¾æ¥"""
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            text = a.get_text(strip=True)[:100]
            if text and not href.startswith(('javascript:', '#', 'mailto:', 'tel:')):
                links.append({'text': text, 'href': href})
        return links[:50]  # é™åˆ¶æ•°é‡

    def _get_images(self, soup, base_url):
        """è·å–å›¾ç‰‡ä¿¡æ¯"""
        from urllib.parse import urljoin

        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src:
                full_url = urljoin(base_url, src)
                alt = img.get('alt', '')
                images.append({'src': full_url, 'alt': alt})
        return images[:20]  # é™åˆ¶æ•°é‡

    def format_output(self, data, format_type='md'):
        """æ ¼å¼åŒ–è¾“å‡ºå†…å®¹"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        if format_type == 'md':
            return self._format_markdown(data, timestamp)
        elif format_type == 'txt':
            return self._format_text(data, timestamp)
        elif format_type == 'json':
            return json.dumps(data, ensure_ascii=False, indent=2)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format_type}")

    def _format_markdown(self, data, timestamp):
        """ç”Ÿæˆ Markdown æ ¼å¼"""
        md = []
        md.append(f"# {data['title']}\n")
        md.append(f"**URL:** {data['url']}")
        md.append(f"**é‡‡é›†æ—¶é—´:** {timestamp}\n")
        md.append("---\n")

        if data['meta'].get('description'):
            md.append(f"**æ‘˜è¦:** {data['meta']['description']}\n")

        if data['headings']:
            md.append("## ç›®å½•\n")
            for h in data['headings']:
                indent = "  " * (h['level'] - 1)
                md.append(f"{indent}- {h['text']}")
            md.append("\n---\n")

        md.append("## æ­£æ–‡å†…å®¹\n")
        md.append(data['main_content'])
        md.append("\n---\n")

        if data['links']:
            md.append(f"## å‚è€ƒé“¾æ¥ ({len(data['links'])} ä¸ª)\n")
            for link in data['links'][:20]:
                md.append(f"- [{link['text']}]({link['href']})")
            md.append("\n")

        if data['images']:
            md.append(f"## ç›¸å…³å›¾ç‰‡ ({len(data['images'])} å¼ )\n")
            for img in data['images'][:10]:
                alt = img['alt'] or 'å›¾ç‰‡'
                md.append(f"- ![{alt}]({img['src']})")
            md.append("\n")

        return '\n'.join(md)

    def _format_text(self, data, timestamp):
        """ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼"""
        txt = []
        txt.append(f"æ ‡é¢˜: {data['title']}")
        txt.append(f"URL: {data['url']}")
        txt.append(f"é‡‡é›†æ—¶é—´: {timestamp}")
        txt.append("=" * 60 + "\n")

        if data['meta'].get('description'):
            txt.append(f"æ‘˜è¦: {data['meta']['description']}\n")

        if data['headings']:
            txt.append("ç›®å½•:")
            for h in data['headings']:
                indent = "  " * (h['level'] - 1)
                txt.append(f"{indent}- {h['text']}")
            txt.append("\n" + "-" * 60 + "\n")

        txt.append("æ­£æ–‡å†…å®¹:")
        txt.append("-" * 60)
        txt.append(data['main_content'])
        txt.append("\n" + "-" * 60 + "\n")

        if data['links']:
            txt.append(f"å‚è€ƒé“¾æ¥ ({len(data['links'])} ä¸ª):")
            for link in data['links'][:20]:
                txt.append(f"  - {link['text']}: {link['href']}")
            txt.append("\n")

        return '\n'.join(txt)

    def get_filename(self, url, format_type):
        """ç”Ÿæˆæ–‡ä»¶å"""
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        clean_title = ''.join(c for c in domain if c.isalnum() or c in '-_')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        ext = {'md': '.md', 'txt': '.txt', 'json': '.json'}.get(format_type, '.txt')
        return f"{clean_title}_{timestamp}{ext}"


def ask_save_location():
    """è¯¢é—®ç”¨æˆ·ä¿å­˜ä½ç½®"""
    print("\n" + "=" * 50)
    print("è¯·é€‰æ‹©ä¿å­˜ä½ç½®:")
    print("=" * 50)
    print("1. å½“å‰ç›®å½•")
    print("2. æ¡Œé¢")
    print("3. æ–‡æ¡£æ–‡ä»¶å¤¹")
    print("4. è‡ªå®šä¹‰è·¯å¾„")
    print("=" * 50)

    while True:
        try:
            choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-4): ").strip()

            if choice == '1':
                return Path.cwd()
            elif choice == '2':
                desktop = Path.home() / 'Desktop'
                if desktop.exists():
                    return desktop
                return Path.home() / 'æ¡Œé¢'
            elif choice == '3':
                docs = Path.home() / 'Documents'
                if docs.exists():
                    return docs
                return Path.home() / 'æ–‡æ¡£'
            elif choice == '4':
                custom = input("è¯·è¾“å…¥ä¿å­˜è·¯å¾„: ").strip()
                return Path(custom)
            else:
                print("æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥ (1-4)")
        except (EOFError, KeyboardInterrupt):
            print("\næ“ä½œå·²å–æ¶ˆ")
            sys.exit(0)


def ask_format():
    """è¯¢é—®ç”¨æˆ·ä¿å­˜æ ¼å¼"""
    print("\nè¯·é€‰æ‹©ä¿å­˜æ ¼å¼:")
    print("1. Markdown (.md) - æ¨èï¼Œæ ¼å¼ç¾è§‚")
    print("2. çº¯æ–‡æœ¬ (.txt)")
    print("3. JSON (.json) - ç»“æ„åŒ–æ•°æ®")

    while True:
        choice = input("è¯·è¾“å…¥é€‰é¡¹ (1-3, é»˜è®¤1): ").strip() or "1"

        if choice == '1':
            return 'md'
        elif choice == '2':
            return 'txt'
        elif choice == '3':
            return 'json'
        else:
            print("æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥ (1-3)")


def save_content(content, filename, save_dir):
    """ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶"""
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    filepath = save_dir / filename

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)

    return filepath


def main():
    parser = argparse.ArgumentParser(description='ç½‘é¡µå†…å®¹è¯»å–ä¸æ•´ç†å·¥å…·')
    parser.add_argument('url', help='è¦è¯»å–çš„ç½‘é¡µç½‘å€')
    parser.add_argument('-f', '--format', choices=['md', 'txt', 'json'],
                        default='md', help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: md)')
    parser.add_argument('-o', '--output', help='ä¿å­˜è·¯å¾„ï¼ˆè·³è¿‡è¯¢é—®ï¼‰')

    args = parser.parse_args()

    print(f"\nğŸŒ æ­£åœ¨è¯»å–ç½‘é¡µ: {args.url}")

    # åˆ›å»ºè¯»å–å™¨
    reader = WebReader()

    # è·å–ç½‘é¡µå†…å®¹
    try:
        html, final_url = reader.fetch_page(args.url)
        print("âœ“ ç½‘é¡µè·å–æˆåŠŸ")
    except Exception as e:
        print(f"âœ— {e}")
        return 1

    # è§£æå†…å®¹
    try:
        data = reader.parse_content(html, final_url)
        print(f"âœ“ è§£æå®Œæˆ: {data['title']}")
    except Exception as e:
        print(f"âœ— è§£æå¤±è´¥: {e}")
        return 1

    # è¯¢é—®ä¿å­˜ä½ç½®
    if args.output:
        save_dir = Path(args.output)
    else:
        save_dir = ask_save_location()

    # è¯¢é—®æ ¼å¼ï¼ˆå¦‚æœæ²¡æœ‰æŒ‡å®šï¼‰
    format_type = args.format
    if not args.output:
        format_type = ask_format()

    # æ ¼å¼åŒ–å†…å®¹
    try:
        content = reader.format_output(data, format_type)
        print("âœ“ å†…å®¹æ•´ç†å®Œæˆ")
    except Exception as e:
        print(f"âœ— æ ¼å¼åŒ–å¤±è´¥: {e}")
        return 1

    # ç”Ÿæˆæ–‡ä»¶å
    filename = reader.get_filename(final_url, format_type)

    # ä¿å­˜æ–‡ä»¶
    try:
        filepath = save_content(content, filename, save_dir)
        print(f"\nâœ“ æ–‡ä»¶å·²ä¿å­˜: {filepath}")
        print(f"  å¤§å°: {len(content)} å­—ç¬¦")
        return 0
    except Exception as e:
        print(f"âœ— ä¿å­˜å¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\næ“ä½œå·²å–æ¶ˆ")
        sys.exit(0)
    except Exception as e:
        print(f"\nâœ— é”™è¯¯: {e}")
        sys.exit(1)
